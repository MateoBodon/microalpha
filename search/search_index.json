{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Microalpha","text":"<p>Microalpha is an event-driven research platform for reproducible quantitative strategy development. The engine prioritises leakage-safety, deterministic execution, and rich analytics so researchers can iterate quickly without sacrificing rigor.</p>"},{"location":"#why-microalpha","title":"Why Microalpha?","text":"<ul> <li>Leakage-safe core: strict timestamp ordering, FIFO broker interactions, and lookahead guards.</li> <li>Execution realism: TWAP/impact models and a configurable level-2 limit order book.</li> <li>Reproducible pipelines: manifest metadata, trade logs, and automation-ready CLI.</li> <li>Extensible design: users can plug in new strategies, data handlers, and execution layers.</li> <li>Documentation-first: MkDocs site with invariants, manifests, API references, and runnable demos.</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<ol> <li>Install the package</li> </ol> <p><code>bash    pip install microalpha    # or, for local development    pip install -e \".[dev]\"</code></p> <ol> <li>Run the bundled mean-reversion backtest</li> </ol> <p><code>bash    microalpha run -c configs/meanrev.yaml</code></p> <p>The CLI writes manifests, metrics, equity curves, and trade logs under <code>artifacts/&lt;run_id&gt;/</code>.</p> <ol> <li> <p>Explore further</p> </li> <li> <p>Inspect leakage invariants in Leakage Safety.</p> </li> <li>Review reproducibility workflows in Reproducibility.</li> <li>Extend components using the API Reference.</li> <li>Try the scenarios in Examples.</li> </ol> <p>Use the navigation to dive into leakage guarantees, reproducibility tooling, API surfaces, and runnable examples.</p>"},{"location":"api/","title":"API Reference","text":"<p>This page summarises the primary extension points for building strategies and tooling on top of Microalpha.</p>"},{"location":"api/#runner-microalpharunner","title":"Runner (<code>microalpha.runner</code>)","text":"<ul> <li><code>run_from_config(path: str, override_artifacts_dir: str | None = None) -&gt; dict</code></li> <li>Loads a YAML config, resolves paths, executes the backtest, and returns a manifest-style dictionary containing artifact paths.</li> <li><code>override_artifacts_dir</code> overrides the root output directory without modifying the on-disk YAML.</li> <li><code>prepare_artifacts_dir(cfg_path: Path, config: dict) -&gt; tuple[str, Path]</code></li> <li>Allocates an isolated <code>artifacts/&lt;run_id&gt;</code> directory for each run.</li> </ul>"},{"location":"api/#engine-microalphaengineengine","title":"Engine (<code>microalpha.engine.Engine</code>)","text":"<pre><code>Engine(data, strategy, portfolio, broker, rng: numpy.random.Generator | None = None)\n</code></pre> <ul> <li>Streams <code>MarketEvent</code>s from the data handler, enforces monotonic timestamps, and routes signals/orders/fills between components.</li> <li>Accepts a <code>numpy.random.Generator</code> for reproducibility; randomness is typically consumed by downstream components (e.g., latency models) rather than the engine itself.</li> </ul> <p>Profiling: - Set <code>MICROALPHA_PROFILE=1</code> or pass <code>--profile</code> via the CLI to record a <code>profile.pstats</code> under the active run\u2019s artifact directory.</p>"},{"location":"api/#data-microalphadatacsvdatahandler","title":"Data (<code>microalpha.data.CsvDataHandler</code>)","text":"<pre><code>CsvDataHandler(csv_dir: Path, symbol: str, mode: str = \"exact\")\n</code></pre> <ul> <li>Loads OHLCV CSV data and produces chronological <code>MarketEvent</code>s via <code>stream()</code>.</li> <li><code>get_latest_price(ts)</code> supports <code>\"exact\"</code> and <code>\"ffill\"</code> modes for timestamp alignment.</li> <li><code>get_future_timestamps(start_ts, n)</code> schedules TWAP child orders without lookahead.</li> <li><code>get_recent_prices(symbol, end_ts, lookback)</code> for sizing; <code>get_volume_at(symbol, ts)</code> for VWAP.</li> </ul>"},{"location":"api/#portfolio-microalphaportfolioportfolio","title":"Portfolio (<code>microalpha.portfolio.Portfolio</code>)","text":"<pre><code>Portfolio(data_handler, initial_cash, *, max_exposure=None, max_drawdown_stop=None,\n          turnover_cap=None, kelly_fraction=None, trade_logger=None,\n          capital_policy=None)\n</code></pre> <ul> <li>Tracks cash, inventory, and equity while enforcing exposure/drawdown/turnover limits.</li> <li>Emits fills to the <code>JsonlWriter</code> (<code>trade_logger</code>) so executions are captured in <code>trades.jsonl</code>.</li> <li>Provides <code>on_market</code>, <code>on_signal</code>, and <code>on_fill</code> hooks consumed by the engine.</li> <li>Adds realized PnL attribution per fill (average-cost) under <code>realized_pnl</code> and cumulative <code>cum_realized_pnl</code> in trades.</li> <li>Resolve config-driven capital policies via <code>capital_policy</code> in YAML; the runner instantiates them automatically.</li> </ul>"},{"location":"api/#broker-execution-microalphabroker-microalphaexecution","title":"Broker &amp; Execution (<code>microalpha.broker</code>, <code>microalpha.execution</code>)","text":"<ul> <li><code>SimulatedBroker(executor)</code> \u2013 wraps an <code>Executor</code> and enforces t+1 semantics before returning fills.</li> <li><code>Executor</code> \u2013 base class implementing simple price-impact + commission fills against the <code>DataHandler</code>.</li> <li><code>TWAP</code> \u2013 splits orders evenly across future timestamps supplied by the data handler.</li> <li><code>VWAP</code> \u2013 splits by future-tick volumes; uses <code>DataHandler.get_volume_at</code> if available (requires <code>volume</code> column).</li> <li><code>ImplementationShortfall</code> \u2013 front-loaded geometric schedule controlled by <code>urgency</code>.</li> <li><code>SquareRootImpact</code> / <code>KyleLambda</code> \u2013 stylised impact models for execution cost studies.</li> <li><code>LOBExecution</code> \u2013 routes orders to the in-memory level-2 book (<code>microalpha.lob.LimitOrderBook</code>) with latency simulation.</li> <li>Supply <code>slippage</code> in YAML to enable <code>VolumeSlippageModel</code> without hand-wiring objects.</li> </ul>"},{"location":"api/#logging-microalphaloggingjsonlwriter","title":"Logging (<code>microalpha.logging.JsonlWriter</code>)","text":"<pre><code>JsonlWriter(path: str)\n</code></pre> <ul> <li>Creates parent directories, writes JSON-serialised objects per line, and flushes eagerly so artifacts remain tail-able.</li> </ul> <p>Refer to the module docstrings and tests for deeper examples of composing these components.</p>"},{"location":"api/#capital-allocation-microalphacapital","title":"Capital Allocation (<code>microalpha.capital</code>)","text":"<ul> <li><code>VolatilityScaledPolicy(lookback=20, target_dollar_vol=10000)</code> scales base order qty inversely to recent per-symbol volatility.</li> <li>Pass into <code>Portfolio(..., capital_policy=VolatilityScaledPolicy(...))</code> or declare under <code>capital_policy</code> in YAML to enable per-name risk targeting.</li> </ul>"},{"location":"api/#cli-microalphacli","title":"CLI (<code>microalpha.cli</code>)","text":"<ul> <li><code>microalpha run -c &lt;cfg&gt; [--out DIR] [--profile]</code></li> <li><code>microalpha wfv -c &lt;cfg&gt; [--out DIR] [--profile]</code></li> </ul> <p><code>--out</code> overrides the artifacts root directory; <code>--profile</code> enables cProfile.</p>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>This page documents how to run the bundled micro-benchmark and interpret the results.</p> <ul> <li>Script: <code>benchmarks/bench_engine.py</code></li> <li>Purpose: Measures raw event throughput of the engine and Portfolio wiring under a no-op strategy and zero-cost execution model.</li> </ul>"},{"location":"benchmarks/#running-locally","title":"Running locally","text":"<pre><code>python benchmarks/bench_engine.py\n</code></pre> <p>The harness prints a small JSON with the number of processed events, wall-clock seconds, and events/sec.</p> <p>Example on Apple M2 Pro (32GB, macOS 14.6.1):</p> <pre><code>{\"events\": 1000000, \"sec\": 0.773, \"evps\": 1294141}\n</code></pre> <p>Your numbers will vary by hardware, Python version, and build flags. Use the harness to compare relative changes across code revisions (e.g., after refactoring a tight loop).</p>"},{"location":"benchmarks/#profiling-a-run","title":"Profiling a run","text":"<p>Enable profiling for any CLI run and inspect hotspots with <code>snakeviz</code> or <code>gprof2dot</code>:</p> <pre><code>microalpha run -c configs/meanrev.yaml --profile\n# or\nMICROALPHA_PROFILE=1 microalpha run -c configs/meanrev.yaml\n</code></pre> <p>The engine writes <code>profile.pstats</code> under the active run\u2019s artifact directory:</p> <pre><code>artifacts/&lt;run_id&gt;/profile.pstats\n</code></pre> <p>Open it with <code>snakeviz</code>:</p> <pre><code>pip install snakeviz\nsnakeviz artifacts/&lt;run_id&gt;/profile.pstats\n</code></pre> <p>This integrates with the per-run artifacts so profiles travel alongside metrics and trades.</p>"},{"location":"data_sp500/","title":"Data Inventory: data_sp500/","text":"<p>A quick audit of the bundled S&amp;P 500-style panel (<code>data_sp500/</code>) produced the following snapshot (see <code>reports/data_inventory_sp500.json</code> for the full dump):</p> <ul> <li>936 per-symbol CSVs (<code>timestamp</code>, <code>close</code>, <code>volume</code>).</li> <li>Global coverage: 2005-01-03 through 2024-12-31 (daily frequency).</li> <li>Average length: ~3,752 rows per symbol.</li> <li>Missing <code>volume</code>: 19 tickers exhibit gaps (notably AAL, BPYU, COOP). Clean or forward-fill before running executions that depend on volume.</li> <li>Non-positive <code>volume</code>: 10 tickers contain zero/negative prints (e.g., ABMD, BTU). These should be filtered or corrected during preprocessing.</li> </ul>"},{"location":"data_sp500/#next-step-checks","title":"Next-step Checks","text":"<ul> <li>Validate holiday calendars and ensure no forward-looking dates slip in.</li> <li>Join with a constituent metadata table (sectors, market cap, share count) to enforce universe filters and neutralization.</li> <li>Consider trimming defunct tickers once they delist to avoid unrealistic OOS signals.</li> </ul> <p>The audit was generated via:</p> <pre><code>python3 - &lt;&lt;'PY'\nfrom pathlib import Path\nimport pandas as pd\nimport json\n\nroot = Path('data_sp500')\nfiles = sorted(root.glob('*.csv'))\nsummary = []\nmissing_volume_symbols = []\nnonpos_volume_symbols = []\nfor path in files:\n    df = pd.read_csv(path)\n    date_col = df.columns[0]\n    df[date_col] = pd.to_datetime(df[date_col])\n    summary.append({\n        'symbol': path.stem,\n        'start': df[date_col].min().strftime('%Y-%m-%d'),\n        'end': df[date_col].max().strftime('%Y-%m-%d'),\n        'rows': len(df),\n        'missing_close': int(df['close'].isna().sum()),\n        'missing_volume': int(df['volume'].isna().sum()),\n        'nonpos_volume': int((df['volume'] &lt;= 0).sum()),\n    })\n    if df['volume'].isna().any():\n        missing_volume_symbols.append(path.stem)\n    if (df['volume'] &lt;= 0).any():\n        nonpos_volume_symbols.append(path.stem)\n\nPath('reports/data_inventory_sp500.json').write_text(json.dumps({\n    'total_symbols': len(files),\n    'summary': summary,\n    'missing_volume': missing_volume_symbols,\n    'nonpos_volume': nonpos_volume_symbols,\n}, indent=2))\nPY\n</code></pre> <p>(Already executed and committed so you don\u2019t have to rerun unless the panel changes.)</p>"},{"location":"data_sp500/#enrichment-metadata","title":"Enrichment &amp; Metadata","text":"<p>Before running flagship strategies, clean the raw panel and emit symbol-level metadata:</p> <pre><code>python scripts/augment_sp500.py \\\n    --source data_sp500 \\\n    --dest data_sp500_enriched \\\n    --sector-map metadata/sp500_sector_overrides.csv \\\n    --metadata-output metadata/sp500_enriched.csv \\\n    --summary-output reports/data_sp500_cleaning.json\n</code></pre> <p>The script forward-fills missing volume, clips non-positive prints, and produces:</p> <ul> <li><code>data_sp500_enriched/</code> \u2013 cleaned per-symbol CSVs.</li> <li><code>metadata/sp500_enriched.csv</code> \u2013 ADV, market-cap proxy, sector labels, history summary.</li> <li><code>reports/data_sp500_cleaning.json</code> \u2013 overview of fixes and outstanding issues.</li> </ul> <p>Supply additional sector metadata by appending CSVs via repeated <code>--sector-map</code> arguments (the overrides in <code>metadata/sp500_sector_overrides.csv</code> cover the liquid names bundled with this repo).</p>"},{"location":"examples/","title":"Examples","text":"<p>Kick off experiments quickly using the bundled configuration files and the reproducible sample data under <code>data/sample/</code>.</p>"},{"location":"examples/#1-flagship-momentum-quickstart","title":"1. Flagship momentum quickstart","text":"<pre><code>microalpha run --config configs/flagship_sample.yaml --out artifacts/sample_flagship\nmicroalpha report --artifact-dir artifacts/sample_flagship --summary-out reports/summaries/flagship_mom.md\n</code></pre> <p>The run command generates deterministic artifacts (metrics, bootstrap distribution, exposures, trades) using the new linear+sqrt slippage model, IOC queueing, and covariance-aware allocator. The report command renders a PNG tear sheet plus a Markdown case study.</p>"},{"location":"examples/#2-walk-forward-reality-check-on-the-sample-universe","title":"2. Walk-forward reality check on the sample universe","text":"<pre><code>microalpha wfv --config configs/wfv_flagship_sample.yaml --out artifacts/sample_wfv\nmicroalpha report --artifact-dir artifacts/sample_wfv --summary-out reports/summaries/flagship_mom_wfv.md --title \"Flagship Walk-Forward\"\n</code></pre> <p>This executes a rolling walk-forward with Politis\u2013White bootstrap and writes <code>folds.json</code>, <code>bootstrap.json</code>, <code>exposures.csv</code>, and aggregated metrics for the flagship strategy.</p>"},{"location":"examples/#3-classic-single-asset-examples","title":"3. Classic single-asset examples","text":"<ul> <li>Mean reversion</li> </ul> <p><code>bash   microalpha run --config configs/meanrev.yaml</code></p> <ul> <li>Breakout momentum</li> </ul> <p><code>bash   microalpha run --config configs/breakout.yaml</code></p> <ul> <li>Market making (LOB simulator)</li> </ul> <p><code>bash   microalpha run --config configs/mm.yaml</code></p> <ul> <li>Parameter walk-forward (single asset)</li> </ul> <p><code>bash   microalpha wfv --config configs/wfv_meanrev.yaml</code></p> <p>Each command drops a self-contained artifact directory; you can call <code>microalpha report --artifact-dir &lt;dir&gt;</code> on any of them to render the markdown/PNG bundle.</p>"},{"location":"factors/","title":"Factor Regression Example","text":"<p>Microalpha ships a tiny, fully offline Fama\u2013French three-factor sample under <code>data/factors/ff3_sample.csv</code>. The file contains daily observations for <code>Mkt_RF</code>, <code>SMB</code>, <code>HML</code>, and the risk-free rate expressed in decimal form. You can run a quick factor attribution against any Microalpha artifact with the utility script <code>reports/factors_ff.py</code>:</p> <pre><code>python reports/factors_ff.py artifacts/sample_wfv/&lt;RUN_ID&gt; \\\n  --factors data/factors/ff3_sample.csv \\\n  --output reports/summaries/factors_sample.md\n</code></pre> <p>The script performs an ordinary-least-squares regression of excess portfolio returns on the factor panel, estimating Newey\u2013West HAC standard errors (default lag = 5). Microalpha\u2019s reporting pipeline automatically incorporates the table into <code>reports/summaries/flagship_mom_wfv.md</code> when the factor CSV is present, so the published walk-forward summary highlights factor loadings and alpha quality.</p> <p>Because the sample datasets are bundled, no external downloads or API keys are required\u2014ideal for CI environments and reproducible research notes.</p>"},{"location":"flagship_strategy/","title":"Flagship Strategy Blueprint (Sample Edition)","text":"<p>The flagship package now ships with a reproducible cross-sectional momentum pipeline that demonstrates the full microstructure stack\u2014configurable slippage, borrow accrual, limit-order queueing, covariance allocators, and bootstrap reporting.</p>"},{"location":"flagship_strategy/#dataset","title":"Dataset","text":"<ul> <li>Location: <code>data/sample/</code></li> <li>Files:</li> <li><code>prices_sample.csv</code>: panel of six synthetic equities (~3y of business days).</li> <li><code>prices/</code>: per-symbol CSVs consumed by <code>MultiCsvDataHandler</code>.</li> <li><code>meta_sample.csv</code>: ADV, borrow cost, and spread inputs for slippage/queue modelling.</li> <li><code>rf_sample.csv</code>: daily risk-free rate in basis points.</li> <li><code>universe_sample.csv</code>: monthly eligibility snapshot with sector and liquidity stats.</li> </ul> <p>No external data vendors are required; runs are deterministic.</p>"},{"location":"flagship_strategy/#pipeline-overview","title":"Pipeline Overview","text":"Component Implementation Signals 12-1 sector-neutral momentum (<code>FlagshipMomentumStrategy</code>) Allocation Budgeted risk parity with Ledoit\u2013Wolf fallback (<code>microalpha.allocators</code>) Execution TWAP with IOC queue model, linear+sqrt impact with spread floor Financing Daily borrow accrual from <code>meta_sample.csv</code> Risk Controls Sector caps, exposure heat, ADV turnover clamp Evaluation HAC-adjusted Sharpe, Politis\u2013White bootstrap (stationary blocks)"},{"location":"flagship_strategy/#reproduce-the-single-run-case-study","title":"Reproduce the Single-Run Case Study","text":"<pre><code>make dev          # optional helper -&gt; pip install -e '.[dev]'\nmicroalpha run --config configs/flagship_sample.yaml --out artifacts/sample_flagship\nmicroalpha report --artifact-dir artifacts/sample_flagship\n</code></pre> <ul> <li>Outputs <code>metrics.json</code>, <code>bootstrap.json</code>, <code>exposures.csv</code>, <code>trades.jsonl</code>, and <code>tearsheet.png</code>.</li> <li><code>reports/summaries/flagship_mom.md</code> is refreshed automatically by the <code>report</code> step.</li> </ul>"},{"location":"flagship_strategy/#walk-forward-reality-check","title":"Walk-Forward Reality Check","text":"<pre><code>microalpha wfv --config configs/wfv_flagship_sample.yaml --out artifacts/sample_wfv\nmicroalpha report --artifact-dir artifacts/sample_wfv --summary-out reports/summaries/flagship_mom_wfv.md --title \\\"Flagship Walk-Forward\\\"\n</code></pre> <ul> <li>Sliding window: 252-day train / 63-day test.</li> <li>Grid: <code>{top_frac \u2208 {0.3, 0.4}, skip_months \u2208 {1, 2}}</code>.</li> <li>Stores per-fold metrics, queue-aware execution logs, and bootstrap Sharpe distributions.</li> </ul>"},{"location":"flagship_strategy/#key-metrics-generated","title":"Key Metrics (generated)","text":"<p>Results will vary slightly with config tweaks; the shipped summary documents the canonical run and includes:</p> <ul> <li>Sharpe ratio with HAC standard errors.</li> <li>Calmar / Sortino / turnover.</li> <li>Bootstrap Sharpe histogram + p-value.</li> <li>Top absolute exposure table driven by final holdings.</li> </ul>"},{"location":"flagship_strategy/#extending-beyond-the-sample","title":"Extending Beyond the Sample","text":"<ol> <li>Swap <code>data_path</code> to a directory of per-symbol CSVs (format identical to <code>data/sample/prices/*.csv</code>).</li> <li>Update <code>meta_path</code> with symbol-specific ADV/borrow/spread estimates.</li> <li>Adjust allocator settings via <code>strategy.allocator</code> / <code>allocator_kwargs</code> in the config.</li> <li>Tune queue parameters under <code>exec.queue_*</code> for different liquidity regimes.</li> </ol> <p>The accompanying tests (<code>tests/test_flagship_momentum.py</code>, <code>tests/test_allocators.py</code>, <code>tests/test_reality_check_store.py</code>) codify invariants for momentum selection, covariance allocation, and bootstrap artefact persistence.</p>"},{"location":"leakage-safety/","title":"Leakage Safety","text":"<p>Microalpha enforces a strict \"no-peek\" discipline at every layer of the simulation stack.</p>"},{"location":"leakage-safety/#engine-invariants","title":"Engine invariants","text":"<ul> <li>Monotonic clocks \u2013 the <code>Engine</code> raises <code>LookaheadError</code> if market events arrive out of order. See <code>tests/test_time_ordering.py</code>.</li> <li>t+1 execution \u2013 strategies submit intents at time t and executions occur no earlier than the next event. Verified in <code>tests/test_tplus1_execution.py</code>.</li> <li>Fill ordering \u2013 brokers acknowledge fills only after the active market event has been processed.</li> </ul>"},{"location":"leakage-safety/#portfolio-guards","title":"Portfolio guards","text":"<ul> <li>Signal timestamps \u2013 the portfolio refuses to act on stale signals (<code>tests/test_time_ordering.py::test_strategy_cannot_backdate_signals</code>).</li> <li>Fill timestamps \u2013 fills older than the portfolio clock raise <code>LookaheadError</code>, ensuring fills cannot materialise from the future.</li> </ul>"},{"location":"leakage-safety/#limit-order-book-sequencing","title":"Limit order book sequencing","text":"<p>The <code>LimitOrderBook</code> keeps per-level FIFO queues to ensure first-in-first-out fill priority. <code>tests/test_lob_fifo.py</code> and <code>tests/test_lob_cancel_latency.py</code> cover partial fills, cancel acknowledgements, and latency offsets, guaranteeing orders are matched in arrival order without leaking future liquidity.</p>"},{"location":"leakage-safety/#lob-t1-semantics","title":"LOB t+1 semantics","text":"<p>By default, LOB execution enforces t+1 semantics by shifting the reported <code>FillEvent.timestamp</code> to the next available market timestamp while retaining measured latency fields. This preserves the global no-peek invariant. You can disable this behavior per config with:</p> <pre><code>exec:\n  type: lob\n  lob_tplus1: false\n</code></pre>"},{"location":"leakage-safety/#walk-forward-orchestration","title":"Walk-forward orchestration","text":"<p>During walk-forward validation, the optimizer only uses in-sample data to select parameters. Each fold records train/test windows in the JSON fold summary, providing an audit trail that the optimizer never touches out-of-sample data (<code>tests/test_walkforward.py</code>).</p>"},{"location":"leakage-safety/#statistical-inference-invariants","title":"Statistical inference invariants","text":"<ul> <li>Sharpe statistics use the same deterministic return stream as performance metrics, with optional HAC adjustments (<code>METRICS_HAC_LAGS</code>) that never peek beyond the evaluated window. <code>tests/test_risk_stats.py</code> asserts IID vs HAC behaviour on synthetic AR(1) data and validates block bootstrap coverage.</li> <li>Reality check bootstraps in walk-forward mode rely on stationary/circular block resampling, seeded from the configuration manifest so repeated runs reproduce identical <code>reality_check_pvalue</code> results.</li> </ul> <p>Together, these invariants provide strong protection against accidentally leaking future information into historical tests.</p>"},{"location":"reproducibility/","title":"Reproducibility","text":"<p>Microalpha emits a manifest for every run so you can replay results and audit configuration drift.</p>"},{"location":"reproducibility/#manifest-fields","title":"Manifest fields","text":"<p>Each backtest stores <code>artifacts/&lt;run_id&gt;/manifest.json</code> with:</p> <ul> <li><code>run_id</code> \u2013 UTC timestamp + short Git SHA used to scope outputs.</li> <li><code>microalpha_version</code> \u2013 package version recorded at runtime.</li> <li><code>git_sha</code> \u2013 repository commit used for the run.</li> <li><code>config_sha256</code> \u2013 hash of the raw YAML bytes for integrity checking.</li> <li><code>python</code> \u2013 interpreter version string.</li> <li><code>platform</code> \u2013 OS information.</li> <li><code>numpy_version</code> \u2013 NumPy version active during the run.</li> <li><code>pandas_version</code> \u2013 pandas version active during the run.</li> <li><code>seed</code> \u2013 RNG seed applied to the shared <code>numpy.random.Generator</code>.</li> </ul> <p>Sharpe metrics and walk-forward bootstraps inherit the same seed to keep statistical tests reproducible. Setting environment variables such as <code>METRICS_HAC_LAGS</code> or CLI overrides like <code>--reality-check-method circular</code> will be reflected in the stored manifest/config combination, allowing exact reruns.</p> <p>The manifest is defined in <code>src/microalpha/manifest.py</code> and written by <code>src/microalpha/runner.py</code>.</p> <p>Example snippet:</p> <pre><code>{\n  \"run_id\": \"2024-03-27T14-22-19Z-4f3c1d1\",\n  \"microalpha_version\": \"0.1.1\",\n  \"git_sha\": \"4f3c1d1\",\n  \"config_sha256\": \"2f4b32...\",\n  \"seed\": 7,\n  \"python\": \"3.11.6\",\n  \"platform\": \"Ubuntu 22.04\",\n  \"numpy_version\": \"1.26.4\",\n  \"pandas_version\": \"2.2.2\"\n}\n</code></pre>"},{"location":"reproducibility/#trade-logs","title":"Trade logs","text":"<p>Executions append JSON Lines to <code>artifacts/&lt;run_id&gt;/trades.jsonl</code> via <code>JsonlWriter</code>. Each line captures <code>timestamp</code>, <code>order_id</code>, <code>symbol</code>, <code>side</code>, <code>qty</code>, <code>price</code>, <code>commission</code>, <code>slippage</code>, <code>inventory</code>, and <code>cash</code>, enabling downstream reconciliation and deterministic comparisons (<code>tests/test_trades_jsonl.py</code>).</p> <p><code>metrics.json</code> is also deterministic: returns are computed with <code>ddof=0</code>, Sharpe analytics report <code>sharpe_ratio_se</code>, <code>sharpe_ratio_ci_low/high</code>, and the optional HAC lag count (<code>sharpe_hac_lags</code>). Walk-forward manifests include <code>reality_check_pvalue</code> per fold so inference decisions can be replayed exactly.</p>"},{"location":"reproducibility/#cli-determinism","title":"CLI determinism","text":"<p><code>microalpha run</code> and <code>microalpha wfv</code> construct a single seeded <code>numpy.random.Generator</code> and pass it to the engine, broker, and portfolio to remove duplicate seeding. <code>tests/test_determinism.py</code> verifies repeated runs produce identical manifests, metrics, and trade logs.</p> <p>To reproduce a run:</p> <pre><code>microalpha run -c configs/meanrev.yaml\n</code></pre> <p>Then inspect the artifacts folder recorded in the manifest to fetch equity curves, metrics, and trades.</p> <p>Rolling factor exposures can be regenerated from artifacts using:</p> <pre><code>python reports/factor_exposure.py --equity artifacts/&lt;run-id&gt;/equity_curve.csv \\\n    --factors data/factors/fama_french_daily.csv --window 63 \\\n    --output artifacts/&lt;run-id&gt;/factor_exposures.png\n</code></pre>"},{"location":"reproducibility/#data-sourcing-wrdscrsp","title":"Data sourcing (WRDS/CRSP)","text":"<p>For resume-grade, bias-aware experiments, use WRDS/CRSP daily data adjusted for corporate actions, and include delisted securities to avoid survivorship bias. We recommend a monthly universe selection (e.g., top 1000 by market cap) saved to CSVs (one file per symbol) under <code>data_sp500/</code> or <code>data_wrds/</code> with columns including at least <code>close</code> and a datetime index. Keep raw credentials and data out of the repo; only derived CSVs or aggregated artifacts should be saved.</p>"},{"location":"wrds/","title":"WRDS / CRSP Integration","text":"<p>Microalpha can operate directly on WRDS-hosted CRSP equities data. This guide describes the expected directory layout, mandatory columns, and the licensing caveats you must observe before pointing the engine at production-quality datasets.</p>"},{"location":"wrds/#data-layout","title":"Data Layout","text":"<p>Set the <code>template.data_path</code> field in <code>configs/wfv_flagship_wrds.yaml</code> to a directory containing per-symbol pricing files exported from WRDS/CRSP. Each file can be CSV or Parquet and must be named <code>SYMBOL.ext</code> (upper-case ticker symbols). Microalpha expects the following schema:</p> Column Description Type Frequency <code>date</code> Trading day in ISO format (<code>YYYY-MM-DD</code>) date Daily (NYSE calendar) <code>open</code>, <code>high</code>, <code>low</code>, <code>close</code> Split- and dividend-adjusted prices float Daily <code>volume</code> Shares traded float/int Daily <code>ret</code> Total return for the period (optional; used for QA) float Daily <code>shares_out</code> Shares outstanding (optional; feeds ADV calc) float Daily <p>All files should share the same timezone (New York). Missing trading days must be forward-filled by WRDS; Microalpha will drop dates that are absent in the price file.</p>"},{"location":"wrds/#metadata-universe","title":"Metadata &amp; Universe","text":"<p>Set <code>template.meta_path</code> to a CSV exported from WRDS security master (<code>crsp.stocknames</code>). The file must contain at least:</p> <ul> <li><code>symbol</code> \u2013 ticker symbol (upper-case).</li> <li><code>permno</code> \u2013 CRSP permanent number.</li> <li><code>sic</code> or <code>gics_sector</code> \u2013 sector/industry classification used for turnover limits.</li> <li><code>delist_date</code> \u2013 optional; if present Microalpha will stop trading on the delisting day.</li> </ul> <p>Provide a universe file via <code>strategy.params.universe_path</code>. A minimal file contains <code>symbol</code> and optional <code>sector</code> columns. You can export the CRSP constituents you care about and roll them forward to the testing window (e.g., keep entries where <code>namedt &lt;= test_end &lt; nameendt</code>).</p>"},{"location":"wrds/#survivorship-corporate-actions","title":"Survivorship &amp; Corporate Actions","text":"<ul> <li>Download full-history panels from WRDS to avoid survivorship bias. Do not pre-filter the   security master to only active listings.</li> <li>Use WRDS adjustment factors (e.g., <code>ajex</code>, <code>cfacpr</code>) to back-adjust prices for splits and   distribute dividends via the bundled total-return column.</li> <li>When running walk-forward, ensure any share-class mergers are handled in your universe file.</li> </ul>"},{"location":"wrds/#licensing-access-controls","title":"Licensing &amp; Access Controls","text":"<ul> <li>WRDS/CRSP data is licensed content. Do not commit any of it to this repository or build   artifacts.</li> <li>The provided Makefile target <code>wrds</code> will refuse to run until you replace the placeholder   values in <code>configs/wfv_flagship_wrds.yaml</code>.</li> <li>Store your WRDS credentials outside the repo (environment variables or <code>.pgpass</code>). Microalpha   only consumes local CSV/Parquet exports and does not connect to WRDS directly.</li> </ul>"},{"location":"wrds/#recommended-workflow","title":"Recommended Workflow","text":"<ol> <li>Export CRSP/Compustat data to a local directory (<code>/wrds/crsp/&lt;project&gt;</code>).</li> <li>Update <code>configs/wfv_flagship_wrds.yaml</code> with the absolute paths from step 1.</li> <li>Run <code>make wrds</code> to produce walk-forward artifacts under <code>artifacts/wrds_flagship/</code>.</li> <li>Execute <code>microalpha report --artifact-dir &lt;artifact&gt;</code> to render summaries and visuals.</li> </ol> <p>By keeping raw data outside the repository and documenting every transformation, you maintain a clear audit trail while respecting WRDS licensing requirements.</p>"}]}